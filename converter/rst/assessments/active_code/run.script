#!/usr/bin/env python3

import base64
import glob
import sys
import os
import subprocess
import re
import uuid

sys.path.append('/usr/share/codio/assessments')
from lib.grade import send_partial, send_partial_v2, FORMAT_V2_HTML


def compile_unit_tests():
    unit_tests_compile_result = subprocess.Popen(
        f'javac -d /tmp/active_code/{ex_dir} -cp .guides/lib/*:. -sourcepath .guides/lib:.guides/active_code/{ex_dir} '
        f'.guides/secure/active_code/{ex_dir}/RunestoneTests.java',
        shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True)
    out, error_message = unit_tests_compile_result.communicate()
    return error_message.strip(), unit_tests_compile_result.returncode


def compile_answer():
    answer_compile_result = subprocess.Popen(
        f'javac -d /tmp/active_code/{ex_dir} -cp .guides/lib/*:. -sourcepath .guides/lib:.guides/active_code/{ex_dir} '
        f'.guides/active_code/{ex_dir}/{class_name}.java',
        shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True)
    out, error_message = answer_compile_result.communicate()
    return error_message.strip(), answer_compile_result.returncode


def run_unit_tests():
    subprocess.Popen(f'rm -f *.java', shell=True, stdout=subprocess.PIPE, universal_newlines=True)
    subprocess.Popen(f'ln -s .guides/active_code/{ex_dir}/{class_name}.java {class_name}.java', shell=True,
                     stdout=subprocess.PIPE, universal_newlines=True)

    run = subprocess.Popen(
        f'java -cp .guides/lib/*:/tmp/active_code/{ex_dir} org.junit.runner.JUnitCore RunestoneTests', shell=True,
        stdout=subprocess.PIPE, universal_newlines=True)
    out, runtime_error = run.communicate()

    subprocess.Popen(f'rm -f *.java', shell=True, stdout=subprocess.PIPE, universal_newlines=True)

    results_match = re.finditer(r'Expected: (?P<expected>.*?)Actual: (?P<actual>.*?)Message: (?P<message>.*?)'
                                r'Passed: (?P<passed>.*?)$', out.strip(), flags=re.MULTILINE)
    results = []
    for result in results_match:
        results.append({'expected': result.group('expected').strip(), 'actual': result.group('actual').strip(),
                        'message': result.group('message').strip(), 'passed': result.group('passed').strip()})

    return results, run.returncode


def get_turtle_image():
    image_data = None
    ret = subprocess.Popen(f'java -cp /tmp/active_code/{ex_dir}:.guides/lib/* {class_name}',
                           shell=True, stdout=subprocess.PIPE, universal_newlines=True)
    output, runtime_error = ret.communicate()
    image_match_re = re.compile(r'&ltimg src=\"data:image/(?P<ext>.*?);base64,(?P<data>.*?)\"/>')
    image_match = image_match_re.search(output)
    if image_match:
        for file in glob.glob(f'.guides/active_code/{ex_dir}/turtle_image_*'):
            os.remove(file)
        ext = image_match.group('ext')
        base64image = image_match.group('data')
        image_path = f'.guides/active_code/{ex_dir}/turtle_image_{str(uuid.uuid4())}.{ext}'
        with open(image_path, 'wb') as file:
            file.write(base64.b64decode(base64image))
        image_data = image_match_re.sub(f'<img src="{image_path}" />', output)
    return image_data


def read_file(f_path):
    with open(f_path) as file:
        return file.read()


def grading():
    output = f'{feedback}<h1>Total Grade: {grade}</h1>'
    res = send_partial_v2(grade, output, FORMAT_V2_HTML)
    exit(0 if res else 1)


if __name__ == '__main__':
    error_msg = ''
    ex_dir = ''
    ex_private_path = ''
    code_file_path = ''
    class_name = ''
    feedback = ''

    if len(sys.argv) > 1:
        class_name = sys.argv[1]
        ex_dir = f'{sys.argv[2]}'
        code_file_path = f'.guides/active_code/{ex_dir}/{class_name}.java'
    answer_code = read_file(code_file_path)
    unit_tests_path = f'.guides/secure/active_code/{ex_dir}/RunestoneTests.java'
    if os.path.exists(unit_tests_path):
        unit_tests = read_file(unit_tests_path)
        error_msg, return_code = compile_unit_tests()

        if return_code != 0:
            feedback = error_msg
            grade = 0

        error_msg, return_code = compile_answer()
        if return_code != 0:
            feedback = error_msg
            grade = 0

        if return_code == 0:
            tests_result, ret_code = run_unit_tests()
            total_tests = len(tests_result)
            passed_tests = len([test for test in tests_result if test['passed'] == 'true'])
            grade = round(passed_tests / total_tests * 100)

            for ind, item in enumerate(tests_result, start=1):
                passed = item['passed'] == 'true'
                status = '<span style="color: green;"><b>PASSED</b></span>' if passed \
                    else '<span style="color: red;"><b>FAILED</b></span>'
                expected = item['expected']
                actual = item['actual']
                message = item['message']
                feedback += f'Test {ind} {status}\n<b>Expected:</b>\n{expected}\n<b>Actual:</b>\n{actual}\n' \
                            f'<b>Message:</b>\n{message}\n\n'

        image = get_turtle_image()
        if image is not None:
            feedback += image
        grading()
    else:
        error_msg, return_code = compile_answer()
        if return_code != 0:
            feedback = error_msg
            grade = 0
        else:
            image = get_turtle_image()
            feedback = image.strip()
            grade = 0
        grading()
